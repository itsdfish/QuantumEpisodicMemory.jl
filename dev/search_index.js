var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"api/#Constructors","page":"API","title":"Constructors","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"GQEM","category":"page"},{"location":"api/#QuantumEpisodicMemory.GQEM","page":"API","title":"QuantumEpisodicMemory.GQEM","text":"GQEM{T<:Real} <: AbstractGQEM{T}\n\nA model object for the Generalized Quantum Episodic Memory (GQEM) model of item recognition. In the recognition memory task, subjects study a list of words. In the test phase, three types of words are presented: old words from the study list, new but semantically related words, and new but unrelated words. Subjects are given four sets of instructions\n\ngist: respond \"yes\" to semantically related words (G)\nverbatim: respond \"yes\" to old (i.e. studied) words (V)\ngist + verbatim: respond \"yes\" to semantically related and old words (G ∪ V)\nunrelated: respond \"yes\" to unrelated words (U)\n\nThe law of total probability is violated in experiments, such that Pr(G) + Pr(V) > Pr(G ∪ V). Similarly, the judgments are subadditive: Pr(G) + Pr(V) + Pr(U) > 1. These effects emerge in the GQEM because the memory representations are incompatible, meaning they are represented with different, non-orthogonal bases and evaluated sequentially. As a result, LOTP and additivity do not necessarily hold. \n\nFields\n\nθG::T: angle in radians between the verbatim and gist bases \nθU::T: angle in radians between the verbatim and new unrelated bases \nθψO::T: angle in radians between verbatim basis and the initial state for old words\nθψR::T: angle in radians between verbatim basis and the initial state for related new words \nθψU::T: angle in radians between verbatim basis and the initial state for new unrelated words\n\nExample\n\nusing QuantumEpisodicMemory\n\nθG = -.12\nθU = -1.54\nθψO = -.71\nθψR = -.86\nθψU = 1.26\n\ndist = GQEM(; θG, θU, θψO, θψR, θψU)\npreds = compute_preds(dist)\ntable = to_table(preds)\n\n# violation of LOPT\nsum(table[[\"gist\",\"verbatim\"],:], dims=1) - table[\"gist+verbatim\", :]'\n\nReferences\n\nTrueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.\n\n\n\n\n\n","category":"type"},{"location":"api/#Core-Functions","page":"API","title":"Core Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"compute_preds\nlogpdf\nrand","category":"page"},{"location":"api/#QuantumEpisodicMemory.compute_preds","page":"API","title":"QuantumEpisodicMemory.compute_preds","text":"compute_preds(dist::AbstractGQEM)\n\nReturns a matrix of predictions for the GQEM model. \n\nThe output is organized in a matrix where rows correspond to instructions and  columns correspond to word type:\n\n word type  \ncondition old related unrelated\ngist 0.65 0.65 0.9\nverbatim 0.35 0.35 0.65\nGist + verbatim 0.69 0.69 0.91\nunrelated new 0.9 0.9 1\n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.logpdf","page":"API","title":"Distributions.logpdf","text":"logpdf(dist::AbstractGQEM, n::Union{Int,Array{Int,N}}, data::Array{Int,N})\n\nReturns the log likelihood of the data for the GQEM model.\n\nThe data are  organized in a matrix where rows correspond to instructions and  columns correspond to word type:\n\n word type  \ncondition old related unrelated\ngist 3 5 9\nverbatim 0 1 2\ngist + verbatim 4 1 10\nunrelated new 5 8 2\n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\nn::Union{Int, Array{Int, N}}: the number of trials \ndata::Array{Int, N}: number of \"yes\" responses \n\n\n\n\n\n","category":"function"},{"location":"api/#Base.rand","page":"API","title":"Base.rand","text":"rand(dist::AbstractGQEM, n::Union{Int,Array{Int,N}})\n\nGenerates data from the GQEM model \n\nThe output is organized in a matrix where rows correspond to instructions and  columns correspond to word type:\n\n word type  \ncondition old related unrelated\ngist 3 5 9\nverbatim 0 1 2\ngist + verbatim 4 1 10\nunrelated new 5 8 2\n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"to_table","category":"page"},{"location":"api/#QuantumEpisodicMemory.to_table","page":"API","title":"QuantumEpisodicMemory.to_table","text":"to_table(x)\n\nConverts matrix to table with labeled dimensions and indices. \n\nExample\n\n4×3 Named Matrix{Float64}\ncondition ╲ word type │       old    related  unrelated\n──────────────────────┼────────────────────────────────\ngist                  │  0.690462   0.545336  0.0359636\nverbatim              │  0.575113   0.425675   0.093524\ngist + verbatim       │  0.694898   0.551852  0.0497793\nunrelated new         │  0.455457   0.604619   0.887783\n\n\n\n\n\n","category":"function"},{"location":"api/#Plots","page":"API","title":"Plots","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"plot","category":"page"},{"location":"api/#RecipesBase.plot","page":"API","title":"RecipesBase.plot","text":"plot(dist::AbstractGQEM; font_size = 10, kwargs...)\n\nPlots the projection of an AbstractGQEM model as a 3 x 3 set of unit circles. Across all unit circles,  the bases are the same. However, the state vector varies by row and the basis vector onto which the state vector is projected varies by column. \n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\n\nKeywords\n\nfont_size = 10: font size of the vector labels \nkwargs...: optional keyword arguments passed to the plot functions \n\nExample\n\ndist = GQEM(; \n    θG = -.5,\n    θU = 2,\n    θψO = .90,\n    θψR = .15,\n    θψU = -1.5,\n)\n       \nplot(dist)\n\n\n\n\n\nplot(\n    dist::AbstractGQEM,\n    θψ,\n    θ_basis;\n    state_label = L\"psi\",\n    font_size = 10,\n    kwargs...\n)\n\nPlots the projection from a given state vector onto a given basis vector within a unit circle. \n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\nθψ: the angle of the state vector with respect to the verbatim basis \nθ_basis: the angle of the basis onto which the state vector is projected. The angle   is with respect to the verbatim basis.\n\nKeywords\n\nstate_label = \"L\"psi\": the label of the state vector ket\nfont_size = 10: font size of the vector labels \nkwargs...: optional keyword arguments passed to the plot functions \n\nExample\n\ndist = GQEM(; \n    θG = -.5,\n    θU = 2,\n    θψO = .90,\n    θψR = .15,\n    θψU = -1.5,\n)\n       \nplot(dist, .1, -.5)\n\n\n\n\n\n","category":"function"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"parameter_estimation/#Bayesian-Parameter-Estimation","page":"Parameter Estimation","title":"Bayesian Parameter Estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"In this tutorial, we demonstrate how to perform Bayesian parameter estimation of the GQEM using Pigeons.jl with the Turing.jl interface.  For a description of the decision making task, please see the description in the model overview. ","category":"page"},{"location":"parameter_estimation/#Load-Packages","page":"Parameter Estimation","title":"Load Packages","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"using Pigeons\nusing QuantumEpisodicMemory\nusing Random\nusing Turing\nusing StatsPlots\nRandom.seed!(3320)","category":"page"},{"location":"parameter_estimation/#Generate-Data","page":"Parameter Estimation","title":"Generate Data","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"In the code block below, we generate simulated data for parameter estimation. The first portion of the code block generates a NamedTuple of the parameters. Next, the parameters are passed to the GQEM model constructor. Finally, we generate responses from 100 trials per condition and combine the data inputs into a Tuple named data. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"parms = (\n    θG = -.12,\n    θU = -1.54,\n    θψO = -.71,\n    θψR = -.86,\n    θψU = 1.26,\n)\ndist = GQEM(; parms...)\nn_trials = 100\nresponses = rand(dist, n_trials)\ndata = (n_trials, responses)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"To make the responses easier to interpret, we can use to_table to add labels to the rows and columns. The rows correspond to experimental conditions and columns correspond to word types.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"table = to_table(responses)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"4×3 Named Matrix{Int64}\ncondition ╲ word type │       old    related  unrelated\n──────────────────────┼────────────────────────────────\ngist                  │        70         63          6\nverbatim              │        49         48         10\ngist+verbatim         │        64         49          5\nunrelated new         │        46         63         91","category":"page"},{"location":"parameter_estimation/#Turing-Model","page":"Parameter Estimation","title":"Turing Model","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"In the code block below, the Bayesian model is defined using the @model macro in Turing. The parameters inside the model function follow a Von Mises distribution, which roughly corresponds to a normal distribution on a circle (i.e., the support is -pi pi). For each parameter the mean is set to zero, and the concentration parameter, which is inversely related to the variance, is set to a small number of 10.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"@model function model(data)\n    θG ~ VonMises(0, .1)\n    θU ~ VonMises(0, .1)\n    θψO ~ VonMises(0, .1)\n    θψR ~ VonMises(0, .1)\n    θψU ~ VonMises(0, .1)\n    data ~ GQEM(; θG, θU, θψO, θψR, θψU)\nend","category":"page"},{"location":"parameter_estimation/#Estimate-the-Parameters","page":"Parameter Estimation","title":"Estimate the Parameters","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Quantum models present challenges for many MCMC samplers because they often produce multimodal posterior distributions.  Multimodal distributions can be traced back to the periodic effect of rotating state vectors or bases. As illustrated below, the marginal log likelihood of the data vacillates as theta_U varies across its permissible range. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: )","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"<details>\n<summary><b>Show Code </b></summary>","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"θUs = range(-π, π, length = 300)\nLLs = map(θU -> logpdf(GQEM(; parms..., θU), n_trials, responses), θUs)\nplot(θUs, LLs, xlabel = \"θU\", ylabel = \"LL\", leg=false, grid=false)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"</details>","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"To overcome limitations of common MCMC samplers, we will use parallel tempering with the package Pigeons.jl.  In the code block below, we pass the Turing model with the tuple of data to the function pigeons. Because the GQEM is highly multimodal, we increase the number of chains  from the default of 10 to 20. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"# Estimate parameters\npt = pigeons(\n    target = TuringLogPotential(model(data)), \n    record = [traces], \n    multithreaded = true, \n    n_chains = 20,\n)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"chains = Chains(pt)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The output below shows the mean, standard deviation, effective sample size, and rhat for each of the five parameters. The panel below shows the quantiles of the marginal distributions. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Summary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec \n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Missing \n\n          θG   -0.0883    1.9003    0.1310   209.2796   495.0300    1.0026       missing\n          θU   -0.0527    1.5713    0.0858   404.8390   543.2327    0.9994       missing\n         θψO    0.1158    1.6601    0.1256   188.7431   617.9621    1.0053       missing\n         θψR   -0.0568    1.6559    0.1124   230.9679   527.2334    1.0123       missing\n         θψU   -0.2296    1.5712    0.1082   171.3612   587.1192    1.0227       missing\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n          θG   -3.0715   -0.1495   -0.0539    0.1420    3.0691\n          θU   -1.6493   -1.5724   -1.4849    1.5704    1.6413\n         θψO   -2.4354   -0.7697    0.7051    0.7974    2.4393\n         θψR   -2.3437   -0.8994   -0.7815    0.8831    2.3380\n         θψU   -1.9365   -1.8452   -1.2235    1.2722    1.9279","category":"page"},{"location":"parameter_estimation/#Posterior-Distributions","page":"Parameter Estimation","title":"Posterior Distributions","text":"","category":"section"},{"location":"parameter_estimation/#Mariginal-Posterior-Distributions","page":"Parameter Estimation","title":"Mariginal Posterior Distributions","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The marginal posterior distributions below are highly multimodal, as expected.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"plot(chains, grid = false)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: )","category":"page"},{"location":"parameter_estimation/#Corner-Plots","page":"Parameter Estimation","title":"Corner Plots","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Below, we will use corner plots to assess the ability of Pigeons to recover the data generating parameters. The corner plots are arranged as a matrix such that the subplot in row i and column j is the join posterior distribution of parameter i and parameter j. The blue horizontal and vertical lines represent the true, data-generating parameter values. For each parameter pair, the blue lines intersect on one of the modes, indicating the parameters were correctly recovered. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"pairplot(chains, PairPlots.Truth(parms))","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: )","category":"page"},{"location":"parameter_estimation/#References","page":"Parameter Estimation","title":"References","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Trueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"plots/#Visualizing-The-Model-Predictions","page":"Plots","title":"Visualizing The Model Predictions","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"This tutorial demonstrates how to visualize the predictions of the GQEM. When Plots is loaded into your session along with QuantumEpisodicMemory, a method for the function plot is loaded, allowing one to visualize the model predictions as projections within a unit circle.","category":"page"},{"location":"plots/#Load-Packages","page":"Plots","title":"Load Packages","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"using LaTeXStrings\nusing QuantumEpisodicMemory\nusing Plots","category":"page"},{"location":"plots/#Initialize-Model","page":"Plots","title":"Initialize Model","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"In the code block below, we define a GQEM model. ","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"parms = (\n    θG = -.5,\n    θU = 2,\n    θψO = .90,\n    θψR = .10,\n    θψU = -1.5,\n)\nmodel = GQEM(; parms...)","category":"page"},{"location":"plots/#Generate-Plot","page":"Plots","title":"Generate Plot","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Next, we pass the model object to the plot function to visualize the predictions. Each unit circle consists of the same three bases, but each row has a different state vector, and in each column, the state vector is projected onto a different basis vector. The bases are defined below:  ","category":"page"},{"location":"plots/#Bases","page":"Plots","title":"Bases","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Verbatim basis: boldsymbolchi_V =  ketV = 10^top ketV^perp = 01^top \nGist basis: boldsymbolchi_G =  ketG ketG^perp \nNew Unrelated basis: boldsymbolchi_U =  ketU ketU^perp ","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"Unit circles in each row include the same state vector, shown in red to distinguish them from the basis vectors. Each instruction condition is associated with a unique state vector, defined as:","category":"page"},{"location":"plots/#State-Vectors","page":"Plots","title":"State Vectors","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Old state vector: ketpsi_O\nNew related state vector: ketpsi_R\nNew unrelated state vector: ketpsi_U","category":"page"},{"location":"plots/#Projectors-and-Projections","page":"Plots","title":"Projectors and Projections","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"The projectors are denoted by a dashed black line. By contrast, the projections are denoted by a green, thick arrow. ","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"plot(model)","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"You can also generate a prediction plot for a single condition by specifing the state vector angle and the basis vector angle (relative to the verbatim basis). In the example below, ketpsi_R is projected onto ketG.","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"plot(model, parms.θψR, parms.θG; state_label = L\"\\psi_R\")","category":"page"},{"location":"plots/#References","page":"Plots","title":"References","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Trueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"model_description/#Introduction","page":"Model Description","title":"Introduction","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"This page provides an overview of the Generalized Quantum Episodic Memory (GQEM) model and its implementation in the Julia package QuantumEpisodicMemoryModels. The GQEM is a quantum model of recognition memory which accounts for phenomena that pose challenges to many alternative models. These phenomena include subadditivity (a.k.a. overdispersion) and violations of the law of total probability (LOTP). In what follows, we will introduce a recognition memory task used to study subaddivity and violations of the LOTP, the mechanics of the GQEM model, and illustrate some basic functionality provided by this package.","category":"page"},{"location":"model_description/#Task","page":"Model Description","title":"Task","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"In the recognition memory task, participants study a list of items (e.g., pictures or words) during the learning phase. Subsequently, in the test phase, participants distinguish between previously studied items and two types of new items. The three item types are:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"O: an old item defined as an item in the study list\nR: a new item defined as an item that is related an item in the study list\nU: a new item that is not related to any items in the study list ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Particpants complete the test phase under one of three between-subject conditions:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"V: respond yes to old items (verbatim)\nG: respond yes to new, related items (gist)\nV cup G: respond yes to old items or new, related items (verbatim + gist)\nU: respond yes to new, unrelated items (unrelated)","category":"page"},{"location":"model_description/#Subadditivity","page":"Model Description","title":"Subadditivity","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Classical probability theory requires mutually exclusive and exhaustive events to sum to 1. In the recognition memory task above, subjects are instructed to respond yes to items in three mutually exclusive and exhaustive categories: gist (G), verbatim (V), and new unrelated (U). Thus, for a given item type i in ORU , the judgments summed across the three conditions should be: ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pr(Y_G = 1 mid i) + Pr(Y_V = 1 mid i) + Pr(Y_U = 1 mid i) = 1","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Subadditivity, which occurs when the sum exceeds 1, is frequently observed in recognition memory decisions.","category":"page"},{"location":"model_description/#Model-Description","page":"Model Description","title":"Model Description","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The goal of this section is to introduce the mechanics and concepts underlying the QGEM. Before introducing the GQEM, we will load the QuantumEpisodicMemory package along with packages for plotting and LaTeX support for mathematical support. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"using LaTeXStrings\nusing QuantumEpisodicMemory\nusing Plots\nusing Random\nRandom.seed!(407)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Quantum cognition distinguishes between two types of representations: compatible and incompatible. Compatible representations can be evaluated simultaneously within the same basis. For example, if you can simultaneously think and reason about your political beliefs and those of your friend, you are using a compatible representation. The joint probability distribution is represented with a common basis. However, if you cannot represent the beliefs simultenously, the beliefs are incompatible. As a consequence, they must be evaluated sequentially using a different basis for each. The bases are defined in the same representational space, but are related to each other through a rotation. Conceptually, this is analogous to shifting one's perceptive to reason about another's political beliefs. ","category":"page"},{"location":"model_description/#Bases","page":"Model Description","title":"Bases","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The GQEM model assumes that the features of an item — gist (G), verbatim (V), and unrelated (U) – are incompatible. For this reason, the features are represented in mathbbR^2 with respect to their own bases:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Verbatim basis: boldsymbolchi_V =  ketV = 10^top ketV^perp = 01^top \nGist basis: boldsymbolchi_G =  ketG ketG^perp \nNew Unrelated basis: boldsymbolchi_U =  ketU ketU^perp ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Note that all quantities discussed below are defined relative to this boldsymbolchi_V, which is arbitarily anchored to the standard position.","category":"page"},{"location":"model_description/#State-Vectors","page":"Model Description","title":"State Vectors","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Upon viewing an old, new related, or new unrelated items a person enters a superposition defined by the corresponding state vectors:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Old state vector: ketpsi_O\nNew related state vector: ketpsi_R\nNew unrelated state vector: ketpsi_U","category":"page"},{"location":"model_description/#Parameters","page":"Model Description","title":"Parameters","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The GQEM consists of 5 parameters which describe the angle between the standard basis boldsymbolchi_V =  ketV = 10^top ketV^perp = 01^top  and other two bases and the three state vectors. The parameters are defined as follows:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"theta_G: angle between basis boldsymbolchi_V and boldsymbolchi_G in radians\ntheta_U: angle between basis boldsymbolchi_V and boldsymbolchi_U in radians\ntheta_psi_O: angle between basis boldsymbolchi_V and state vector ketpsi_O in radians\ntheta_psi_R: angle between basis boldsymbolchi_V and state vector ketpsi_R in radians\ntheta_psi_U: angle between basis boldsymbolchi_V and state vector ketpsi_U in radians","category":"page"},{"location":"model_description/#Response-Probabilities","page":"Model Description","title":"Response Probabilities","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The purpose of this section is to provide a geometric illustration of computing response probabilities with the GQEM model. In the code block below, we will begin by setting the value for each parameter, and creating a model object.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"θG = -.5\nθU = 2\nθψO = .90\nθψR = .20\nθψU = -1.5","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Next, we pass the parameters to the GQEM constructor as keyword arguments (order does not matter).","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"dist = GQEM(; θG, θU, θψO, θψR, θψU)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The figure below illustrates geometrically how response probabilities are generated from the GQEM model. In this example, we assume that a person was placed in the gist condition, and is in a superposition state for related new items, ketpsi_R. The probability of responding yes is found by projecting ketpsi_R onto the basis vector ketG. In the figure below, the red vector represents the superposition state ketpsi_R, the green vector represents the projection of ketpsi_R onto ketG and the dashed black line is perpendicular to the projection. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"plot(dist, θψR, θG; state_label = L\"\\psi_R\")","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<details>\n<summary><b>Show Details </b></summary>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The superposition state for related items is obtained by rotating the verbatim basis vector.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"ketpsi_R = mathbbU(theta_psi_R) ketV","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Similarly, the basis state for gist instructions is obtained by rotating the verbatim basis vector.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"ketG = mathbbU(theta_G) ketV","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The projector matrix for basis vector ketG is defined as:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfP = ketG braG","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The probability of responding yes given a related word is defined as the squared magnitude of the projection of ketpsi_R onto ketG:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pr(X = 1 mid R) = lVert mathbfP ketpsi_R rVert^2","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"</details>","category":"page"},{"location":"model_description/#Model-Usage","page":"Model Description","title":"Model Usage","text":"","category":"section"},{"location":"model_description/#Predictions","page":"Model Description","title":"Predictions","text":"","category":"section"},{"location":"model_description/#Response-Probabilities-2","page":"Model Description","title":"Response Probabilities","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The predicted response probabilities are computed via the function compute_pred as shown below. The predictions can be piped to the function to_table to provide row and column names. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"preds = compute_preds(dist) |> to_table","category":"page"},{"location":"model_description/#Subadditivity-2","page":"Model Description","title":"Subadditivity","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Below, the model predicts subadditivity for verbatim and unrelated new items, but not old items. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"sum(preds[[\"gist\", \"verbatim\", \"unrelated new\"],:], dims = 1)","category":"page"},{"location":"model_description/#Generate-Data","page":"Model Description","title":"Generate Data","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The code block below demonstrates how to generate 100 trials for each condition.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"n_trials = 100\ndata = rand(dist, n_trials)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"As before, we can display names for rows and columns to aid in the interpretation of the data. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"to_table(data)","category":"page"},{"location":"model_description/#Log-Likelihood","page":"Model Description","title":"Log Likelihood","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Finally, the code block below shows how to compute the log likelihood of the data using the function logpdf.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"logpdf(dist, n_trials, data)","category":"page"},{"location":"model_description/#References","page":"Model Description","title":"References","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Trueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is a Julia implementation of the Generalized Quantum Episodic Memory model. For more details, use the menu on the left panel to navigate the documentation. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There are two ways to install QuantumEpisodicMemory. The first method is to enter package mode via ] and run the following in package mode:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add https://github.com/itsdfish/QuantumEpisodicMemory.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"The second method involves to add the package to your local environment via this private registry. Follow the instructions in the README to add the registry. Next, activate your local environment and paste the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add QuantumEpisodicMemory","category":"page"},{"location":"","page":"Home","title":"Home","text":"One benefit of the second approach is that it allows you to specify compatibility boundries on the version.  ","category":"page"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following example illustrates how to generate predicted response probabilities from the GQEM model. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using QuantumEpisodicMemory\n\ndist = GQEM(; \n  θG = -.12,\n  θU = -1.54,\n  θψO = -.71,\n  θψR = -.86,\n  θψU = 1.26,\n)\npreds = compute_preds(dist)\ntable = to_table(preds)","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Trueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"model_comparison/#Bayesian-Model-Comparison","page":"Model Comparison","title":"Bayesian Model Comparison","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"In this tutorial, we demonstrate how to perform Bayesian model comparison of two variants of the GQEM using Pigeons.jl with the Turing.jl interface.  For a description of the decision making task, please see the description in the model overview. ","category":"page"},{"location":"model_comparison/#Bayes-Factor","page":"Model Comparison","title":"Bayes Factor","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"Before proceeding to the code, we provide a brief overview of the Bayes factor. Readers who are familiar with Bayes factors can skip this section. In Bayesian model comparison, the Bayes factor allows one to compare the probability of the data under two different models while taking into account model flexibility stemming all sources, including the number of parameters, functional form, and prior distribution. Thus, it provides a way to balance model fit and model flexibility into a single index. One important fact to keep in mind is that Bayes factors can be sensitive to the choice prior distributions over parameters. Sensitivity to prior distributions over parameters might be desireable depending on one's goals and knowledge of the models under consideration. ","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"The Bayes factor is the likelihood of the data mathbfY = lefty_1y_2 dots y_nright under model mathcalM_i vs. model mathcalM_j. The relationship between the Bayes Factor and the posterior of odds of mathcalM_i vs. mathcalM_j can be stated as:","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"fracpi(mathcalM_i mid mathbfY)pi(mathcalM_j mid mathbfY) = fracpi(mathcalM_i)pi(mathcalM_j) mathrmBF_ij","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"The term on the left hand side is the posterior odds of mathcalM_i vs. mathcalM_j, pi is the posterior probability, the first term on the right hand side is the prior odds of mathcalM_i vs. mathcalM_j, and mathrmBF_ij is the Bayes factor for mathcalM_i vs. mathcalM_j.  In the equation above, mathrmBF_ij functions as a conversion factor between prior odds and posterior odds. Thus,  the Bayes factor is as the factor by which prior odds must be updated in light of the data. This interpretation is important because demonstrates that the prior odds should be updated by the same factor even if there is disagreement over the prior odds. The Bayes factor can also be written as the ratio of marginal likelihoods as follows: ","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"mathrmBF_ij = fracf(mathbfY mid mathcalM_i)f(mathbfY mid mathcalM_j),","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"where f is the likelihood function of mathcalM_i, and the marginal likelihood of mathcalM_i is given by:","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"f(mathbfY mid mathcalM_i) = int_boldsymbolthetain boldsymbolTheta_i f(mathbfY mid boldsymboltheta mathcalM_i) pi(boldsymboltheta mid mathcalM_i) d boldsymboltheta.","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"In the equation above, boldsymbolTheta_i is the parameter space for mathcalM_i and boldsymboltheta in boldsymbolTheta is a vector of parameters. Under this interpretation, the marginal likelihood represents its average prior predictive ability of of mathcalM_i. One benefit of the Bayes factor is that the marginal likelihood accounts for model flexibility because the density of the prior distribution must be \"rationed\" across the parameter space (i.e., must integrate to 1). Consequentially, the predictions of a model with a diffuse distribution in a high dimensional parameter space will be penalized due to its low prior density. ","category":"page"},{"location":"model_comparison/#Load-Packages","page":"Model Comparison","title":"Load Packages","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"using Pigeons\nusing QuantumEpisodicMemory\nusing Random\nusing Turing\nRandom.seed!(65)","category":"page"},{"location":"model_comparison/#Models","page":"Model Comparison","title":"Models","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"We will compare two versions of the GQEM model: full model with no contraints on the parameters, and a restricted model with constraints on the basis angles.","category":"page"},{"location":"model_comparison/#Full-Model","page":"Model Comparison","title":"Full Model","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"The full model places no constraints on the angle parameters. Each angle parameter has the following prior distribution:","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"theta_i sim mathrmVonMises(0 10)","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"THe Von Mises distribution roughly corresponds to a normal distribution on the circumference of a circle (i.e., the support is -pi pi). For each parameter the mean is set to zero, and the concentration parameter, which is inversely related to the variance, is set to a small number of 10. In the code block below, the Bayesian model is defined using the @model macro in Turing. ","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"@model function full_model(data)\n    θG ~ VonMises(0, .1)\n    θU ~ VonMises(0, .1)\n    θψO ~ VonMises(0, .1)\n    θψR ~ VonMises(0, .1)\n    θψU ~ VonMises(0, .1)\n    data ~ GQEM(; θG, θU, θψO, θψR, θψU)\nend","category":"page"},{"location":"model_comparison/#Restricted-Model","page":"Model Comparison","title":"Restricted Model","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"The restricted model is identical to the full model with exception of the following constraint: the superposition state vectors must be equal. Thus,","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"theta sim mathrmVonMises(0 1)","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"theta_psi_O = theta_psi_R = theta_psi_U = theta","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"The model in the code block below imposes this constraint:","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"@model function restricted_model(data)\n    θG ~ VonMises(0, .1)\n    θU ~ VonMises(0, .1)\n    θ ~ VonMises(0, .1)\n    data ~ GQEM(; θG, θU, θψO = θ, θψR = θ, θψU = θ)\nend","category":"page"},{"location":"model_comparison/#Generate-Data","page":"Model Comparison","title":"Generate Data","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"In the code block below, we generate simulated data for parameter estimation. The first portion of the code block generates a NamedTuple of the parameters. Next, the parameters are passed to the GQEM model constructor. Finally, we generate responses from 100 trials per condition and combine the data inputs into a Tuple named data. ","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"parms = (\n    θG = -.50,\n    θU = 1.54,\n    θψO = .15,\n    θψR = -.10,\n    θψU = .05,\n)\ndist = GQEM(; parms...)\nn_trials = 100\nresponses = rand(dist, n_trials)\ndata = (n_trials, responses)","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"To make the responses easier to interpret, we can use to_table to add labels to the rows and columns. The rows correspond to experimental conditions and columns correspond to word types.","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"table = to_table(responses)","category":"page"},{"location":"model_comparison/#Compute-Marginal-Log-Likelihoods","page":"Model Comparison","title":"Compute Marginal Log Likelihoods","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"Now that the data have been generated and the models have been specified, we can compare the models. The first step in computing the Bayes factor is to estimate the marginal log likelihood for each model. To achieve this end, we will pass data and each model to the function pigeons. The code blocks for each model and their traces are shown below.","category":"page"},{"location":"model_comparison/#Full-Model-2","page":"Model Comparison","title":"Full Model","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"pt_full = pigeons(target=TuringLogPotential(\n    full_model(data)), \n    record = [traces], \n    multithreaded = true, \n    n_chains = 20,\n)","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"────────────────────────────────────────────────────────────────────────────\n  scans        Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) \n────────── ────────── ────────── ────────── ────────── ────────── ──────────\n        2       6.07       -151   9.64e-15       0.68      0.937      0.993 \n        4          5      -64.4    0.00402      0.737      0.984      0.999 \n        8       4.77      -45.2      0.313      0.749      0.992      0.999 \n       16       5.02      -42.5      0.472      0.736      0.988      0.999 \n       32       5.63      -45.5      0.417      0.704      0.996      0.999 \n       64       5.62      -44.1      0.561      0.704      0.998      0.999 \n      128       5.77        -44      0.572      0.696      0.997          1 \n      256        5.4      -44.2      0.638      0.716      0.997      0.999 \n      512        5.5      -44.1      0.664       0.71      0.998      0.999 \n 1.02e+03       5.45      -44.1      0.674      0.713      0.999          1 \n────────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"model_comparison/#Restricted-Model-2","page":"Model Comparison","title":"Restricted Model","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"pt_restricted = pigeons(target=TuringLogPotential(\n    restricted_model(data)), \n    record = [traces], \n    multithreaded = true, \n    n_chains = 20,\n)","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"────────────────────────────────────────────────────────────────────────────\n  scans        Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) \n────────── ────────── ────────── ────────── ────────── ────────── ──────────\n        2          5       -140   3.58e-14      0.737        0.9      0.995 \n        4        3.5      -64.8    0.00747      0.816          1          1 \n        8       4.64      -55.2      0.327      0.756          1          1 \n       16       3.85      -53.3      0.588      0.797      0.993          1 \n       32       3.92      -54.8      0.546      0.793          1          1 \n       64       4.38      -54.9      0.602       0.77      0.998          1 \n      128       4.08      -54.9       0.67      0.785      0.999          1 \n      256       4.24        -55      0.663      0.777          1          1 \n      512       4.21      -54.5      0.743      0.778      0.999          1 \n 1.02e+03       4.16      -54.7      0.747      0.781      0.999          1 \n────────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"model_comparison/#Compute-Bayes-Factor","page":"Model Comparison","title":"Compute Bayes Factor","text":"","category":"section"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"We can compute the Bayes factor by calling stepping_stone on the Pigeons objects for each model, and exponentiating the difference. For ease of intepretation, the Bayes factor is converted to log_10 units. The output below shows that the data are more than 4 orders of magnitude more likely under the full model compared to the restricted model. ","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"ml_full = stepping_stone(pt_full)\nml_restricted = stepping_stone(pt_restricted)\nBF = log10(exp(ml_full - ml_restricted))","category":"page"},{"location":"model_comparison/","page":"Model Comparison","title":"Model Comparison","text":"4.577669561628219","category":"page"}]
}
