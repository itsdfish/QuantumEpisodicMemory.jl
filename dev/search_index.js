var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"api/#Constructors","page":"API","title":"Constructors","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"GQEM","category":"page"},{"location":"api/#QuantumEpisodicMemory.GQEM","page":"API","title":"QuantumEpisodicMemory.GQEM","text":"GQEM{T<:Real} <: AbstractGQEM{T}\n\nA model object for the Generalized Quantum Episodic Memory (GQEM) model of item recognition. In the recognition memory task, subjects study a list of words. In the test phase, three types of words are presented: old words from the study list, new but semantically related words, and new but unrelated words. Subjects are given four sets of instructions\n\ngist: respond \"yes\" to semantically related words (G)\nverbatim: respond \"yes\" to old (i.e. studied) words (V)\ngist + verbatim: respond \"yes\" to semantically related and old words (G ∪ V)\nunrelated: respond \"yes\" to unrelated words (U)\n\nThe law of total probability is violated in experiments, such that Pr(G) + Pr(V) > P(G ∪ V). Similarly, the judgments are subadditive: Pr(G) + Pr(V) + Pr(U) > 1. These effects emerge in the GQEM because the memory representations are incompatible, meaning they are represented with different, non-orthogonal bases and evaluated sequentially. As a result, LOTP and additivity do not necessarily hold. \n\nFields\n\nθG::T: angle in radians between the verbatim and gist bases \nθU::T: angle in radians between the verbatim and new unrelated bases \nθψO::T: angle in radians between verbatim basis and the initial state for old words\nθψR::T: angle in radians between verbatim basis and the initial state for related new words \nθψU::T: angle in radians between verbatim basis and the initial state for new unrelated words\n\nExample\n\nusing QuantumEpisodicMemory\n\nθG = -.12\nθU = -1.54\nθψO = -.71\nθψR = -.86\nθψU = 1.26\n\ndist = GQEM(; θG, θU, θψO, θψR, θψU)\npreds = compute_preds(dist)\ntable = to_table(preds)\n\n# violation of LOPT\nsum(table[[\"gist\",\"verbatim\"],:], dims=1) - table[\"gist+verbatim\", :]'\n\nReferences\n\nTrueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.\n\n\n\n\n\n","category":"type"},{"location":"api/#Core-Functions","page":"API","title":"Core Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"compute_preds\nlogpdf\nrand","category":"page"},{"location":"api/#QuantumEpisodicMemory.compute_preds","page":"API","title":"QuantumEpisodicMemory.compute_preds","text":"compute_preds(dist::AbstractGQEM)\n\nReturns a matrix of predictions for the GQEM model. \n\nThe output is organized in a matrix where rows correspond to instructions and  columns correspond to word type:\n\n word type  \ncondition old related unrelated\ngist 0.65 0.65 0.9\nverbatim 0.35 0.35 0.65\nGist + verbatim 0.69 0.69 0.91\nunrelated new 0.9 0.9 1\n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.logpdf","page":"API","title":"Distributions.logpdf","text":"logpdf(dist::AbstractGQEM, n::Union{Int,Array{Int,N}}, data::Array{Int,N})\n\nReturns the log likelihood of the data for the GQEM model.\n\nThe data are  organized in a matrix where rows correspond to instructions and  columns correspond to word type:\n\n word type  \ncondition old related unrelated\ngist 3 5 9\nverbatim 0 1 2\ngist + verbatim 4 1 10\nunrelated new 5 8 2\n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\nn::Union{Int, Array{Int, N}}: the number of trials \ndata::Array{Int, N}: number of \"yes\" responses \n\n\n\n\n\n","category":"function"},{"location":"api/#Base.rand","page":"API","title":"Base.rand","text":"rand(dist::AbstractGQEM, n::Union{Int,Array{Int,N}})\n\nGenerates data from the GQEM model \n\nThe output is organized in a matrix where rows correspond to instructions and  columns correspond to word type:\n\n word type  \ncondition old related unrelated\ngist 3 5 9\nverbatim 0 1 2\ngist + verbatim 4 1 10\nunrelated new 5 8 2\n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"to_table","category":"page"},{"location":"api/#QuantumEpisodicMemory.to_table","page":"API","title":"QuantumEpisodicMemory.to_table","text":"to_table(x)\n\nConverts matrix to table with labeled dimensions and indices. \n\nExample\n\n4×3 Named Matrix{Float64}\ncondition ╲ word type │       old    related  unrelated\n──────────────────────┼────────────────────────────────\ngist                  │  0.690462   0.545336  0.0359636\nverbatim              │  0.575113   0.425675   0.093524\ngist + verbatim       │  0.694898   0.551852  0.0497793\nunrelated new         │  0.455457   0.604619   0.887783\n\n\n\n\n\n","category":"function"},{"location":"api/#Plots","page":"API","title":"Plots","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"plot","category":"page"},{"location":"api/#RecipesBase.plot","page":"API","title":"RecipesBase.plot","text":"plot(dist::AbstractGQEM; font_size = 10, kwargs...)\n\nPlots the projection of an AbstractGQEM model as a 3 x 3 set of unit circles. Across all unit circles,  the bases are the same. However, the state vector varies by row and the basis vector onto which the state vector is projected varies by column. \n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\n\nKeywords\n\nfont_size = 10: font size of the vector labels \nkwargs...: optional keyword arguments passed to the plot functions \n\nExample\n\ndist = GQEM(; \n    θG = -.5,\n    θU = 2,\n    θψO = .90,\n    θψR = .15,\n    θψU = -1.5,\n)\n       \nplot(dist)\n\n\n\n\n\nplot(\n    dist::AbstractGQEM,\n    θψ,\n    θ_basis;\n    state_label = L\"psi\",\n    font_size = 10,\n    kwargs...\n)\n\nPlots the projection from a given state vector onto a given basis vector within a unit circle. \n\nArguments\n\ndist::AbstractGQEM: a GQEM distribution object\nθψ: the angle of the state vector with respect to the verbatim basis \nθ_basis: the angle of the basis onto which the state vector is projected. The angle   is with respect to the verbatim basis.\n\nKeywords\n\nstate_label = \"L\"psi\": the label of the state vector ket\nfont_size = 10: font size of the vector labels \nkwargs...: optional keyword arguments passed to the plot functions \n\nExample\n\ndist = GQEM(; \n    θG = -.5,\n    θU = 2,\n    θψO = .90,\n    θψR = .15,\n    θψU = -1.5,\n)\n       \nplot(dist, .1, -.5)\n\n\n\n\n\n","category":"function"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"parameter_estimation/#Bayesian-Parameter-Estimation","page":"Parameter Estimation","title":"Bayesian Parameter Estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"In this tutorial, we demonstrate how to perform Bayesian parameter estimation of the GQEM using Pigeons.jl with the Turing.jl interface.  For a description of the decision making task, please see the description in the model overview. ","category":"page"},{"location":"parameter_estimation/#Load-Packages","page":"Parameter Estimation","title":"Load Packages","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"using Pigeons\nusing QuantumEpisodicMemory\nusing Random\nusing Turing\nusing StatsPlots\nRandom.seed!(3320)","category":"page"},{"location":"parameter_estimation/#Generate-Data","page":"Parameter Estimation","title":"Generate Data","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"In the code block below, we will create a model object and generate 2 simulated responses from 100 simulated subjects for a total of 200 responses. For this model, we assume that the probability of a true preference state RR is relatively high, and the probability of other preference states decreases as they become more difference from RR:","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"parms = (\n    θG = -.12,\n    θU = -1.54,\n    θψO = -.71,\n    θψR = -.86,\n    θψU = 1.26,\n)\ndist = GQEM(; parms...)\nn_trials = 1000\nresponses = rand(dist, n_trials)\ndata = (n_trials, responses)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"table = to_table(responses)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"4×3 Named Matrix{Int64}\ncondition ╲ word type │       old    related  unrelated\n──────────────────────┼────────────────────────────────\ngist                  │        70         63          6\nverbatim              │        49         48         10\ngist+verbatim         │        64         49          5\nunrelated new         │        46         63         91","category":"page"},{"location":"parameter_estimation/#The-Turing-Model","page":"Parameter Estimation","title":"The Turing Model","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"@model function model(data)\n    θG ~ VonMises(0, .1)\n    θU ~ VonMises(0, .1)\n    θψO ~ VonMises(0, .1)\n    θψR ~ VonMises(0, .1)\n    θψU ~ VonMises(0, .1)\n    data ~ GQEM(; θG, θU, θψO, θψR, θψU)\nend","category":"page"},{"location":"parameter_estimation/#Estimate-the-Parameters","page":"Parameter Estimation","title":"Estimate the Parameters","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"# Estimate parameters\npt = pigeons(target=TuringLogPotential(\n    model(data)), \n    record = [traces], \n    multithreaded = true, \n    n_chains = 20,\n)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"For ease of intepretation, we will convert the numerical indices of preference vector mathbfp to more informative labeled indices. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"chains = Chains(pt)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The output below shows the mean, standard deviation, effective sample size, and rhat for each of the five parameters. The pannel below shows the quantiles of the marginal distributions. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Summary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec \n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Missing \n\n          θG   -0.0883    1.9003    0.1310   209.2796   495.0300    1.0026       missing\n          θU   -0.0527    1.5713    0.0858   404.8390   543.2327    0.9994       missing\n         θψO    0.1158    1.6601    0.1256   188.7431   617.9621    1.0053       missing\n         θψR   -0.0568    1.6559    0.1124   230.9679   527.2334    1.0123       missing\n         θψU   -0.2296    1.5712    0.1082   171.3612   587.1192    1.0227       missing\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n          θG   -3.0715   -0.1495   -0.0539    0.1420    3.0691\n          θU   -1.6493   -1.5724   -1.4849    1.5704    1.6413\n         θψO   -2.4354   -0.7697    0.7051    0.7974    2.4393\n         θψR   -2.3437   -0.8994   -0.7815    0.8831    2.3380\n         θψU   -1.9365   -1.8452   -1.2235    1.2722    1.9279","category":"page"},{"location":"parameter_estimation/#Evaluation","page":"Parameter Estimation","title":"Evaluation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"It is important to verify that the chains converged. We see that the chains converged according to hatr leq 105, and the trace plots below show that the chains look like \"hairy caterpillars\", which indicates the chains did not get stuck. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"plot(chains, grid = false)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: )","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"pairplot(chains, PairPlots.Truth(parms))","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: )","category":"page"},{"location":"parameter_estimation/#References","page":"Parameter Estimation","title":"References","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Birnbaum, M. H., & Quispe-Torreblanca, E. G. (2018). TEMAP2. R: True and error model analysis program in R. Judgment and Decision Making, 13(5), 428-440.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Lee, M. D. (2018). Bayesian methods for analyzing true-and-error models. Judgment and Decision Making, 13(6), 622-635.","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"plots/#Visualizing-The-Model-Predictions","page":"Plots","title":"Visualizing The Model Predictions","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"This tutorial demonstrates how to visualize the predictions of the GQEM. When Plots is loaded into your session along with QuantumEpisodicMemory, a method for the function plot is loaded, allowing one to visualize the model predictions as projections within a unit circle.","category":"page"},{"location":"plots/#Load-Packages","page":"Plots","title":"Load Packages","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"The first step is to load the required packages. You will need to install each package in your local environment in order to run the code locally. We will also set a random number generator so that the results are reproducible.","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"using QuantumEpisodicMemory\nusing Plots","category":"page"},{"location":"plots/#Initialize-Model","page":"Plots","title":"Initialize Model","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"In the code block below, we define a GQEM model. ","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"model = GQEM(; \n    θG = -.5,\n    θU = 2,\n    θψO = .90,\n    θψR = .10,\n    θψU = -1.5,\n)","category":"page"},{"location":"plots/#Generate-Plot","page":"Plots","title":"Generate Plot","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Next, we pass the model object to the plot function to visualize the predictions. Each unit circle consists of the same three bases, but each row has a different state vector, and in each column, the state vector is projected onto a different basis vector. The bases are defined below:  ","category":"page"},{"location":"plots/#Bases","page":"Plots","title":"Bases","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Verbatim basis: boldsymbolchi_V =  ketV = 10^top ketV^perp = 01^top \nGist basis: boldsymbolchi_G =  ketG ketG^perp \nNew Unrelated basis: boldsymbolchi_U =  ketU ketU^perp ","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"Unit circles in each row include the same state vector, shown in red to distinguish them from the basis vectors. Each instruction condition is associated with a unique state vector, defined as:","category":"page"},{"location":"plots/#State-Vectors","page":"Plots","title":"State Vectors","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Old state vector: ketpsi_O\nNew related state vector: ketpsi_R\nNew unrelated state vector: ketpsi_U","category":"page"},{"location":"plots/#Projectors-and-Projections","page":"Plots","title":"Projectors and Projections","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"The projectors are denoted by a dashed black line. By contrast, the projections are denoted by a green, thick arrow. ","category":"page"},{"location":"plots/","page":"Plots","title":"Plots","text":"plot(model)","category":"page"},{"location":"plots/#References","page":"Plots","title":"References","text":"","category":"section"},{"location":"plots/","page":"Plots","title":"Plots","text":"Trueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"model_description/#Introduction","page":"Model Description","title":"Introduction","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"This documentation provides an overview of the Generalized Quantum Episodic Memory (GQEM) model and its implementation in the Julia package QuantumEpisodicMemoryModels. The GQEM is a quantum model of item recognition memory which accounts for subadditivity (a.k.a. overdispersion) and violations of the law of total probability. In what follows, we will introduce the task, the mechanics of the model, and illustrate some basic functionality using QuantumEpisodicMemoryModels.","category":"page"},{"location":"model_description/#Task","page":"Model Description","title":"Task","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"In the recognition memory task, participants study a list of items (e.g., pictures or words) during the learning phase. Subsequently, in the test phase, participants distinguish between previously studied items and two types of new items. The three item types are:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"O: old items defined as items in the study list\nR: new items that are related any item in the study list\nU: new items that are not related to any items in the study list ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Particpants complete the test phase under one of three between-subject conditions:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"V: respond yes to old items (verbatim)\nG: respond yes to new, related items (gist)\nV cup G: respond yes to old items or new, related items (verbatim + gist)","category":"page"},{"location":"model_description/#Law-of-Total-Probability","page":"Model Description","title":"Law of Total Probability","text":"","category":"section"},{"location":"model_description/#Subadditivity","page":"Model Description","title":"Subadditivity","text":"","category":"section"},{"location":"model_description/#Model-Description","page":"Model Description","title":"Model Description","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The goal of this section is to introduce the mechanics and concepts underlying the QGEM. Before introducing the GQEM, we will load the QuantumEpisodicMemory package along with packages for plotting and LaTeX support for mathematical support. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"using LaTeXStrings\nusing QuantumEpisodicMemory\nusing Plots\nusing Random\nRandom.seed!(407)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Quantum cognition distinguishes between two types of representations: compatible and incompatible. Compatible representations can be evaluated simultaneously within the same basis. For example, if you can simultaneously think and reason about your political beliefs and those of your friend, you are using a compatible representation. The joint probability distribution is represented with a common basis. However, if you cannot represent the beliefs simultenously, the beliefs are incompatible. As a consequence, they must be evaluated sequentially using a different basis for each. The bases are defined in the same representational space, but are related to each other through a rotation. Conceptually, this is analogous to shifting one's perceptive to reason about another's political beliefs. ","category":"page"},{"location":"model_description/#Bases","page":"Model Description","title":"Bases","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The GQEM model assumes that the features of an item — gist (G), verbatim (V), and unrelated (U) – are incompatible. For this reason, the features are represented in mathbbR^2 with respect to their own bases:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Verbatim basis: boldsymbolchi_V =  ketV = 10^top ketV^perp = 01^top \nGist basis: boldsymbolchi_G =  ketG ketG^perp \nNew Unrelated basis: boldsymbolchi_U =  ketU ketU^perp ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Note that all quantities discussed below are defined relative to this boldsymbolchi_V, which is arbitarily anchored to the standard position.","category":"page"},{"location":"model_description/#State-Vectors","page":"Model Description","title":"State Vectors","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Upon viewing an old, new related, or new unrelated items a person enters a superposition defined by the corresponding state vectors:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Old state vector: ketpsi_O\nNew related state vector: ketpsi_R\nNew unrelated state vector: ketpsi_U","category":"page"},{"location":"model_description/#Parameters","page":"Model Description","title":"Parameters","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The GQEM consists of 5 angle parameters which describe the relationship between the standard basis boldsymbolchi_V =  ketV = 10^top ketV^perp = 01^top  and other two bases and the three state vectors. The parameters are defined as follows:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"theta_G: angle between basis boldsymbolchi_V and boldsymbolchi_G in radians\ntheta_U: angle between basis boldsymbolchi_V and boldsymbolchi_U in radians\ntheta_psi_O: angle between basis boldsymbolchi_V and state vector ketpsi_O in radians\ntheta_psi_R: angle between basis boldsymbolchi_V and state vector ketpsi_R in radians\ntheta_psi_U: angle between basis boldsymbolchi_V and state vector ketpsi_U in radians","category":"page"},{"location":"model_description/#Response-Probabilities","page":"Model Description","title":"Response Probabilities","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The purpose of this section is to provide a geometric illustration of computing response probabilities with the GQEM model. In the code block below, we will begin by setting the value for each parameter.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"θG = -.5\nθU = 2\nθψO = .90\nθψR = .20\nθψU = -1.5","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Next, we pass the parameters to the GQEM constructor as keyword arguments (order does not matter).","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"dist = GQEM(; θG, θU, θψO, θψR, θψU)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The figure below illustrates how response probabilities are generated from the GQEM model. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"plot(dist, θψR, θG; state_label = L\"\\psi_R\")","category":"page"},{"location":"model_description/#Model-Usage","page":"Model Description","title":"Model Usage","text":"","category":"section"},{"location":"model_description/#Predictions","page":"Model Description","title":"Predictions","text":"","category":"section"},{"location":"model_description/#Response-Probabilities-2","page":"Model Description","title":"Response Probabilities","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The predicted response probabilities are computed via the function compute_pred as shown below. The predictions can be piped to the function to_table to provide row and column names. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"preds = compute_preds(dist) |> to_table","category":"page"},{"location":"model_description/#Subadditivity-2","page":"Model Description","title":"Subadditivity","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The predictions for subadditivity can be computed by summing across the item times for each instruction condition. Below, the model predicts subadditivity for verbatim and unrelated new items, but not old items. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"sum(preds[[\"gist\", \"verbatim\", \"unrelated new\"],:], dims = 1)","category":"page"},{"location":"model_description/#Generate-Data","page":"Model Description","title":"Generate Data","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The code block below demonstrates how to generate 100 trials for each condition.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"n_trials = 100\ndata = rand(dist, n_trials)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"As before, we can display names for rows and columns to aid in the interpretation of the data. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"to_table(data)","category":"page"},{"location":"model_description/#Log-Likelihood","page":"Model Description","title":"Log Likelihood","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Finally, the code block below shows how to compute the log likelihood of the data using the function logpdf.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"logpdf(dist, n_trials, data)","category":"page"},{"location":"model_description/#References","page":"Model Description","title":"References","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Trueblood, J. S., & Hemmer, P. (2017). The generalized quantum episodic memory model. Cognitive Science, 41(8), 2089-2125.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"https://raw.githubusercontent.com/itsdfish/QuantumEpisodicMemory.jl/refs/heads/main/docs/logo/logo_readme.png\" alt=\"drawing\" width=\"900\"/>","category":"page"},{"location":"#QuantumEpisodicMemory.jl","page":"Home","title":"QuantumEpisodicMemory.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for QuantumEpisodicMemory.jl","category":"page"}]
}
